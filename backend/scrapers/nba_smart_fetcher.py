import asyncio
from playwright.async_api import async_playwright
import json
import logging
from typing import Dict, Any, Optional, List
import random

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NBASmartFetcher:
    """
    A genius fetcher that uses Playwright to intercept valid API calls 
    generated by the official NBA website, bypassing bot detection.
    """
    
    def __init__(self):
        self.results = {}
        
    async def fetch_stats(self, target_endpoints: List[str]):
        """
        Navigates to NBA stats pages and intercepts the API responses.
        """
        async with async_playwright() as p:
            # Launch browser with specific arguments to avoid detection
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--no-sandbox',
                    '--disable-setuid-sandbox'
                ]
            )
            
            # Create context with realistic user agent and viewport
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                locale='en-US',
                timezone_id='America/New_York'
            )
            
            # Add init script to mask webdriver
            await context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)
            
            page = await context.new_page()
            
            # Set up request interception
            async def handle_response(response):
                url = response.url
                # Check if this is one of our target API endpoints
                for endpoint in target_endpoints:
                    if endpoint in url and "stats.nba.com/stats" in url:
                        try:
                            data = await response.json()
                            if 'resultSets' in data:
                                logger.info(f"âœ… Intercepted data for {endpoint}")
                                self.results[endpoint] = data
                        except Exception as e:
                            pass

            page.on("response", handle_response)
            
            # We need to visit a page that triggers these API calls.
            # The "Traditional" stats page triggers 'leaguedashplayerstats'
            logger.info("ğŸ€ Navigating to NBA Stats home to warm up...")
            await page.goto("https://www.nba.com/stats", wait_until="networkidle")
            
            # List of pages to visit to trigger specific endpoints
            pages_to_visit = [
                {
                    "url": "https://www.nba.com/stats/players/traditional",
                    "wait_for": "leaguedashplayerstats"
                },
                {
                    "url": "https://www.nba.com/stats/players/clutch-traditional",
                    "wait_for": "leaguedashplayerclutch"
                },
                {
                    "url": "https://www.nba.com/stats/players/hustle",
                    "wait_for": "leaguehustlestatsplayer"
                },
                {
                    "url": "https://www.nba.com/stats/teams/traditional",
                    "wait_for": "leaguedashteamstats"
                }
            ]
            
            for page_config in pages_to_visit:
                target = page_config["wait_for"]
                
                # Skip if we already captured it
                if target in self.results:
                    continue
                    
                logger.info(f"ğŸ“ Visiting {page_config['url']} to capture {target}...")
                
                try:
                    await page.goto(page_config["url"], wait_until="domcontentloaded")
                    
                    # Scroll down to trigger lazy loading if needed
                    await page.evaluate("window.scrollTo(0, 500)")
                    
                    # Wait for the network response to be captured
                    try:
                        # Wait up to 10 seconds for the specific response we want
                        await page.wait_for_response(
                            lambda response: target in response.url and response.status == 200,
                            timeout=10000
                        )
                        logger.info(f"âœ¨ Captured {target} successfully!")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Timeout waiting for {target}, but might have been captured globally.")
                        
                    # Random delay to be nice
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    logger.error(f"âŒ Error visiting {page_config['url']}: {e}")

            await browser.close()
            return self.results

    def save_results(self):
        """Save intercepted data to JSON files"""
        timestamp = "latest"
        
        for endpoint, data in self.results.items():
            filename = f"nba_data_{endpoint}_{timestamp}.json"
            with open(filename, 'w') as f:
                json.dump(data, f, indent=2)
            logger.info(f"ğŸ’¾ Saved {endpoint} data to {filename}")
            
            # Print a sneak peek
            if 'resultSets' in data and len(data['resultSets']) > 0:
                headers = data['resultSets'][0]['headers']
                row_count = len(data['resultSets'][0]['rowSet'])
                logger.info(f"ğŸ“Š {endpoint}: {row_count} rows, {len(headers)} columns")

async def main():
    print("ğŸ§  Starting Smart NBA Data Interceptor...")
    print("This approach uses a real browser to browse NBA.com and captures the data it receives.")
    
    fetcher = NBASmartFetcher()
    
    targets = [
        "leaguedashplayerstats",
        "leaguedashplayerclutch",
        "leaguehustlestatsplayer",
        "leaguedashteamstats",
        "leaguedashptdefend"
    ]
    
    data = await fetcher.fetch_stats(targets)
    
    if data:
        print(f"\nğŸ‰ Success! Captured {len(data)} datasets.")
        fetcher.save_results()
    else:
        print("\nâŒ Failed to capture data. The site might have detected the automation.")

if __name__ == "__main__":
    asyncio.run(main())
